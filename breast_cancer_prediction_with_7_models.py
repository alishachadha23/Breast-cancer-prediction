# -*- coding: utf-8 -*-
"""breast cancer prediction- with 7 models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HqGMo2fBw4tPWoqh6H2Y025NPvLamep4
"""

#   Breast Cancer Prediction - 7 Machine Learning Models

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, classification_report,
                             confusion_matrix, roc_auc_score, roc_curve)

# --- 7 Models ---
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier


from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target   # 0 = malignant, 1 = benign

print("Dataset shape:", df.shape)
print("\nTarget distribution:\n", df['target'].value_counts())
print("\nFirst 5 rows:\n", df.head())

# ============================================================
# 3. PREPROCESSING
# ============================================================
# If your CSV has a diagnosis column like 'M'/'B', uncomment:
# df['target'] = df['diagnosis'].map({'M': 0, 'B': 1})
# df.drop(columns=['diagnosis', 'id'], inplace=True, errors='ignore')

# Check for missing values
print("\nMissing values:\n", df.isnull().sum().sum())
df.dropna(inplace=True)

X = df.drop('target', axis=1)
y = df['target']

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Feature Scaling
scaler = StandardScaler()
X_train_sc = scaler.fit_transform(X_train)
X_test_sc  = scaler.transform(X_test)

print(f"\nTraining samples : {X_train.shape[0]}")
print(f"Test samples     : {X_test.shape[0]}")

# ============================================================
# 4. DEFINE 7 MODELS
# ============================================================
models = {
    "Logistic Regression"      : LogisticRegression(max_iter=10000, random_state=42),
    "Decision Tree"            : DecisionTreeClassifier(random_state=42),
    "Random Forest"            : RandomForestClassifier(n_estimators=100, random_state=42),
    "Gradient Boosting"        : GradientBoostingClassifier(n_estimators=100, random_state=42),
    "AdaBoost"                 : AdaBoostClassifier(n_estimators=100, random_state=42),
    "Support Vector Machine"   : SVC(probability=True, random_state=42),
    "K-Nearest Neighbors"      : KNeighborsClassifier(n_neighbors=5),
}

# ============================================================
# 5. TRAIN, EVALUATE & COMPARE
# ============================================================
results = []

print("\n" + "="*65)
print(f"{'Model':<28} {'Accuracy':>10} {'ROC-AUC':>10} {'CV Mean':>10}")
print("="*65)

for name, model in models.items():
    # Use scaled data for distance/margin-based models
    if name in ["Logistic Regression", "Support Vector Machine", "K-Nearest Neighbors"]:
        model.fit(X_train_sc, y_train)
        y_pred  = model.predict(X_test_sc)
        y_proba = model.predict_proba(X_test_sc)[:, 1]
        cv_scores = cross_val_score(model, X_train_sc, y_train, cv=5, scoring='accuracy')
    else:
        model.fit(X_train, y_train)
        y_pred  = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')

    acc     = accuracy_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_proba)
    cv_mean = cv_scores.mean()

    results.append({
        "Model"   : name,
        "Accuracy": round(acc * 100, 2),
        "ROC-AUC" : round(roc_auc, 4),
        "CV Mean" : round(cv_mean * 100, 2),
        "y_pred"  : y_pred,
        "y_proba" : y_proba,
    })

    print(f"{name:<28} {acc*100:>9.2f}% {roc_auc:>10.4f} {cv_mean*100:>9.2f}%")

print("="*65)

# Results DataFrame
results_df = pd.DataFrame(results)[["Model", "Accuracy", "ROC-AUC", "CV Mean"]]
print("\nSummary Table:\n", results_df.sort_values("Accuracy", ascending=False).to_string(index=False))

# ============================================================
# 6. VISUALISATIONS
# ============================================================
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle("Breast Cancer Prediction – Model Comparison", fontsize=16, fontweight='bold')

# --- 6a. Accuracy Bar Chart ---
ax1 = axes[0, 0]
colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(results)))
bars = ax1.barh([r["Model"] for r in results],
                [r["Accuracy"] for r in results], color=colors)
ax1.set_xlabel("Accuracy (%)")
ax1.set_title("Model Accuracy Comparison")
ax1.set_xlim(80, 102)
for bar, r in zip(bars, results):
    ax1.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,
             f'{r["Accuracy"]}%', va='center', fontsize=9)

# --- 6b. ROC Curves ---
ax2 = axes[0, 1]
for r in results:
    fpr, tpr, _ = roc_curve(y_test, r["y_proba"])
    ax2.plot(fpr, tpr, label=f'{r["Model"]} (AUC={r["ROC-AUC"]:.3f})')
ax2.plot([0,1],[0,1],'k--', label='Random Classifier')
ax2.set_xlabel("False Positive Rate")
ax2.set_ylabel("True Positive Rate")
ax2.set_title("ROC Curves")
ax2.legend(fontsize=7, loc='lower right')

# --- 6c. CV Score Comparison ---
ax3 = axes[1, 0]
ax3.bar([r["Model"] for r in results],
        [r["CV Mean"] for r in results],
        color='steelblue', edgecolor='black')
ax3.set_ylabel("CV Accuracy (%)")
ax3.set_title("5-Fold Cross-Validation Accuracy")
ax3.set_xticklabels([r["Model"] for r in results], rotation=30, ha='right', fontsize=8)
ax3.set_ylim(80, 102)

# --- 6d. Confusion Matrix for Best Model ---
best = max(results, key=lambda r: r["Accuracy"])
ax4 = axes[1, 1]
cm = confusion_matrix(y_test, best["y_pred"])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax4,
            xticklabels=['Malignant','Benign'],
            yticklabels=['Malignant','Benign'])
ax4.set_title(f'Confusion Matrix – Best Model\n({best["Model"]}, Acc={best["Accuracy"]}%)')
ax4.set_xlabel("Predicted")
ax4.set_ylabel("Actual")

plt.tight_layout()
plt.savefig("model_comparison.png", dpi=150, bbox_inches='tight')
plt.show()
print("\nPlot saved as 'model_comparison.png'")

# ============================================================
# 7. DETAILED REPORT FOR BEST MODEL
# ============================================================
print(f"\n{'='*50}")
print(f"Best Model: {best['Model']}  |  Accuracy: {best['Accuracy']}%")
print(f"{'='*50}")
print(classification_report(y_test, best["y_pred"],
                             target_names=["Malignant (0)", "Benign (1)"]))